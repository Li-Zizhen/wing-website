@inproceedings{li-etal-2025-dna,
 abstract = {The acceleration of Large Language Models (LLMs) research has opened up new possibilities for evaluating generated text. Though LLMs serve as scalable and economical evaluators, how reliable these evaluators is still under-explored. Prior research efforts in the meta-evaluation of LLMs as judges limit the prompting of an LLM to a single use to obtain a final evaluation decision. They then compute the agreement between LLMsâ€™ outputs and human labels. This lacks interpretability in understanding the evaluation capability of LLMs. In light of this challenge, we propose DnA-Eval, which breaks down the evaluation process into decomposition and aggregation stages based on pedagogical practices. Our experiments show that it not only provides a more interpretable window for how well LLMs evaluate, but also leads to improvements up to 39.6% for different LLMs on a variety of meta-evaluation benchmarks.},
 address = {Abu Dhabi},
 author = {Li, Minzhi  and
Liu, Zhengyuan  and
Deng, Shumin  and
Joty, Shafiq  and
Chen, Nancy  and
Kan, Min-Yen},
 booktitle = {Proceedings of the 31st International Conference on Computational Linguistics},
 editor = "Rambow, Owen  and
      Wanner, Leo  and
      Apidianaki, Marianna  and
      Al-Khalifa, Hend  and
      Eugenio, Barbara Di  and
      Schockaert, Steven",
 month = {January},
 pages = {2277--2290},
 publisher = {Association for Computational Linguistics},
 title = {{D}n{A}-Eval: Enhancing Large Language Model Evaluation through Decomposition and Aggregation},
 url = {https://aclanthology.org/2025.coling-main.156/},
 year = {2025}
}
