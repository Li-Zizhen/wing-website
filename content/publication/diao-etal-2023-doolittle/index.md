---
title: 'Doolittle: Benchmarks and Corpora for Academic Writing Formalization'
authors:
- Shizhe Diao
- Yongyu Lei
- liangming
- Tianqing Fang
- Wangchunshu Zhou
- Sedrick Keh
- min
- Tong Zhang
date: '2023-12-01'
publishDate: '2024-07-06T02:22:24.582277Z'
publication_types:
- paper-conference
publication: '*Proceedings of the 2023 Conference on Empirical Methods in Natural
  Language Processing*'
doi: 10.18653/v1/2023.emnlp-main.809
abstract: Improving the quality of academic writing is a meaningful but challenging
  task. Conventional methods of language refinement focus on narrow, specific linguistic
  features within isolated sentences, such as grammatical errors and improper word
  use. We propose a more general task, Academic Writing Formalization (AWF), to improve
  the overall quality of formal academic writing at the paragraph level. We formulate
  this language refinement task as a formal text style transfer task which transfers
  informal-academic text to formal-academic and contribute a large-scale non-parallel
  dataset, Doolittle, for this purpose. Concurrently, we apply a method named metric-oriented
  reinforcement learning (MORL) to two large language models (LLM) where we incorporate
  different levels of automatic feedback into the training process. Our experiments
  reveal that existing text transfer models and grammatical error correction models
  address certain aspects of AWF but still have a significant performance gap compared
  to human performance. Meanwhile, language models fine-tuned with our MORL method
  exhibit considerably improved performance, rivaling the latest chatbot ChatGPT,
  but still have a non-negligible gap compared to the ground truth formal-academic
  texts in Doolittle.
links:
- name: URL
  url: https://aclanthology.org/2023.emnlp-main.809
---
