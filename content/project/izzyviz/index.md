---
title: Visualising Large Language Model Activations
summary: This project develops a visualizer utilising attention scores to enhance transformer interpretability.
abstract: "In the field of natural language processing, large and complex neural networks, particularly transformers, have become essential due to their effectiveness in handling text data. However, despite their success, current methods for visualizing how activations of specific network units influence model outputs remain underdeveloped. This gap hinders interpretability and limits researchers’ and practitioners’ understanding of neural network behaviors during training and inference.

This project aims to address this issue by creating a robust visualizer tailored for transformer architectures. By processing training or test data, this project seeks to track and analyze changes in attention scores and other structural components of the transformer network, visualizing their evolution within individual input sentences during inference. The project involves developing innovative visualization conventions to compactly represent the high-dimensional neural network processes in a manner that is analytical and accessible, through the usage of static diagrams."

tags: ["LLM", "visualisation", "explanability"]
year: 2024
# Talk start and end times.
#   End time can optionally be hidden by prefixing the line with `#`.
date: "2024-05-17"
date_end: "2025-05-01"
all_day: true

# Is this a featured project? (true/false)
featured: true
#image:
#  caption: 'Image credit: Jesse Gozali Prabawa'
#  focal_point: Right
url_pdf: ""
url_slides: "https://docs.google.com/presentation/d/15wejdRa-Yb2N5sJKRZkJYy-jw-0FM2LDpwLCSztOwqo/edit?usp=sharing"
url_code: "https://github.com/lxz333/IzzyViz"

# Markdown Slides (optional).
#   Associate this talk with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides = "example-slides"` references `content/slides/example-slides.md`.
#   Otherwise, set `slides = ""`.
# slides:

authors: ["xizi", "esther", "min"]
---

In the field of natural language processing, large and complex neural networks, particularly transformers, have become essential due to their effectiveness in handling text data. However, despite their success, current methods for visualizing how activations of specific network units influence model outputs remain underdeveloped. This gap hinders interpretability and limits researchers’ and practitioners’ understanding of neural network behaviors during training and inference.

This project aims to address this issue by creating a robust visualizer tailored for transformer architectures. By processing training or test data, this project seeks to track and analyze changes in attention scores and other structural components of the transformer network, visualizing their evolution within individual input sentences during inference. The project involves developing innovative visualization conventions to compactly represent the high-dimensional neural network processes in a manner that is analytical and accessible, through the usage of static diagrams.
